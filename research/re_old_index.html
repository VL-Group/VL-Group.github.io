<!DOCTYPE html>
<html>

<head>
  <!-- <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script> -->
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Research</title>
  <meta name="description" content="VLG: Research">
  <link rel="canonical" href="index.html">
  <link rel="stylesheet" href="../css/bootstrap.min.css">
  <link rel="stylesheet" href="../css/main.css">
  <link rel="shortcut icon" type="image/x-icon" href="../images/assests/favicon.ico">

</head>

<style>
  h3 {
    counter-reset: paper;
  }

  h4::before {
    counter-increment: paper;
    content: counter(paper)". ";
  }

  h4 {
    font-weight: bold;
  }
</style>


<body>

  <div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="navbar-background-container">
      <div class="container-fluid">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar-collapse-1"
            aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>

          <a class="navbar-brand" href="../index.html">Center for Future Media, UESTC</a>
        </div>
        <div class="collapse navbar-collapse" id="navbar-collapse-1">
          <ul class="nav navbar-nav navbar-right">
            <li><a href="../index.html" class="hvr-underline-from-center">Home</a></li>
            <li><a href="index.html" class="hvr-underline-from-center">Research</a></li>
            <li><a href="../members/index.html" class="hvr-underline-from-center">Members</a></li>
            <li><a href="../publications/index.html" class="hvr-underline-from-center">Publications</a></li>
            <li><a href="../seminars/index.html" class="hvr-underline-from-center">Seminars</a></li>
          </ul>
        </div>
      </div>
    </div>
  </div>

  <div class="container-fluid">
    <div class="row">
      <div id="textid" class="col-sm-12 research-img">
        <center>
          <h1>Research</h1>
        </center>

        <h3 id="work">Parts of our achievements</h3>

        <h4>Visual Question Answering </h4>

        <p>Visual Question Answering is that given an image or a video and a related question, the proposed algorithm
          can answer the question correctly based on the visual information. Most existing methods are mainly based on
          recurrent neural networks (RNNs) with attention. But they are time-consuming and having difficulties in
          modeling long range dependencies due to the sequential nature of RNNs. </p>
        <p>
          <img src="../images/research/vqa.png" />
        </p>
        <p>To tackle this issue, we propose a new architecture, Positional Self-Attention with Co-Attention (PSAC),
          which does not require RNNs for video question answering. The proposed Positional Self-Attention Block can
          calculate the response at each position by attending to all positions within the same sequence and then add
          representations of absolute positions. Therefore, PSAC can exploit the global dependencies of question and
          temporal information in the video and make the process of question and video encoding executed in parallel.
        </p>
        This <a href="http://staff.ustc.edu.cn/~hexn/papers/aaai19-VideoQA.pdf"> paper</a> is accepted as an oral paper
        by AAAI-2019.

        <h4> Face Aging </h4>
        <p>Face aging and rejuvenation aims to predict the
          face of a person at different ages. While tremendous progresses
          have been made in this topic, there are two central problems
          remaining largely unsolved:<li>the majority of prior works
            require sequential training data, which is very rare in real
            scenarios;</li>
          <li> how to simultaneously render aging face and
            preserve personality.</li>
        </p>
        <p>
          <img src="../images/research/face.png" />
        </p>
        <p>To tackle these issues, we
          develop a novel dual conditional GANs (AgeGAN) mechanism,
          which enables face aging and rejuvenation to be trained from
          multiple sets of unlabeled face images with different ages. In
          our architecture, the primal conditional GAN transforms a face
          image to other ages based on the age condition, while the
          dual conditional GAN learns to invert the task. Hence a loss
          function that accounts for the reconstruction error of images can
          preserve the personal identity, while the discriminators on the
          generated images learn the transition patterns (e.g., the shape and
          texture changes between age groups) and guide the generation
          of age-specific photo-realistic faces. </p>

        <p>We further improve our
          networks, termed AgeGAN++, in which we share the weights
          between the primal part and the dual part to ensure a more
          stable training process. Moreover, in order to get more sensible
          results, a representation disentanglement component is integrated
          with the latent facial representation, and a novel enhanced
          discriminator is applied on the generated images of AgeGAN++.
          Experimental results on two public datasets demonstrate the
          appealing performance of the proposed methods by comparing
          with the state-of-the-art methods. </p>


        <h4> Image retrival </h4>
        Our research interests are in hashing and quantization methods for fast image retrieval.
        <p>
          <img src="https://i.loli.net/2019/09/22/jST1hmMRCquo9Vs.png" />
        </p>
        <p>The figure shows our Deep Recurrent Quantization (DRQ) model which is trained once and can generate
          sequential binary codes. It is the first to combine recurrent network and quantization, which significantly
          reduces the codebook size. The code length can be easily controlled by adjusting the number of recurrent
          iterations. Experimental results on the benchark datasets show that our model achieves comparable or even
          better performance compared with the state-of-the-arts for image retrieval. But it requires much less number
          of parameters and training times. This work is accepted as a conference paper in IJCAI-19. <a
            href="https://www.ijcai.org/proceedings/2019/128">[Paper link]</a>. <a
            href="https://github.com/cfm-uestc/DRQ">[Source code]</a>.</p>
        Other works:
        <li>Deep Progressive Quantization (accepted in IJCAI-19): <a
            href="https://www.ijcai.org/proceedings/2019/102">[Paper link]</a>. <a
            href="https://github.com/cfm-uestc/DPQ">[Source code]</a>.</li>






        <h3 id="work">Exploring</h3>
        <h4> Video object segmentation</h4>

        <p>
          Video Object Segmentation is a task that separating a foreground object from a video sequence. We focus on
          semi-supervised VOS in which the ground truth segmentation masks of one or multiple objects are given for the
          first frame in a video. VOS is a fundamental task
          in computer vision, with important applications including video editing, robotics, and self-driving cars.
        </p>
        <p>
          <img src="../images/research/segmentation.png" />
        </p>
        <h4> Visual-and-language Navigation </h4>
        <p> VLN requires an embodied agent to follow natural language instructions to navigate from a starting pose to a
          goal location in the Matterport3D Simulator(a new large-scale visual RL simulation environment).</p><br>
        <table border="0" align="center" cellpadding="0" cellspacing="0">
          <tr>
            <td valign="top">
              <img src="../images/research/VLN.png" style="width: 300px;  border: 10px" /></td>
            <td valign="top">
              <img src="../images/research/VLN2.png" style="width: 250px;  border: 10px" /></td>
            <td valign="top">
              <img src="../images/research/VLN3.png" style="width: 300px;  border: 10px" /></td>
          </tr>
        </table>
        <h4> Adversarial Attack </h4>
        <p><img src="../images/research/FGSM.png" alt="" style="width: 450px; float: left; border: 10px" /></p>
        <p>Recent studies show that deep neural network are vulnerable to adversarial attacks in the form of subtle
          perturbations to inputs that lead a model to predict incorrect outputs. In the picture left, we can't see any
          difference between two images, but DNN really fail. For our group, It's really intersting to understand why it
          will happen. There are also many other intersting application, explore it and make fun!
          <br><i>If you are interested in this area, welcome to follow our work <a href="https://arxiv.org/abs/2007.06765">Patch-wise Attack for Fooling Deep Neural Network</a> (accepted by ECCV2020).<br>
        </p>
      </div>

    </div>
  </div>

  <div id="footer" class="panel">
    <div class="panel-footer">
      <div class="container-fluid">
        <div class="row">
          <center>
            <div class="col-sm-12">
              Contact: Room A305, innovation center, University of Electronic Science and Technology of China (<a
                href="https://www.amap.com/search?query=%E7%94%B5%E5%AD%90%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%A6%E6%B8%85%E6%B0%B4%E6%B2%B3%E6%A0%A1%E5%8C%BA%E5%88%9B%E6%96%B0%E4%B8%AD%E5%BF%83&city=510100&geoobj=103.851502%7C30.542145%7C104.389832%7C30.782536&zoom=12">Maps</a>)
              <br />
              If there are any bugs, please contact <a
                href="mailto:wh.pyjnqd@gmail.com">wh.pyjnqd at gmail dot com</a>. Thank you!
              (2019-9-25) <br />
          </center>
        </div>
      </div>
    </div>
  </div>
  </div>



  <script src="../js/jquery.min.js"></script>
  <script src="../js/bootstrap.min.js"></script>


</body>

</html>